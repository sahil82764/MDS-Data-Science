{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24853,"status":"ok","timestamp":1729602021619,"user":{"displayName":"Hamid Usefi","userId":"10186078069234504560"},"user_tz":150},"id":"P2ZkSLUVLBWM","outputId":"60a1b589-68f6-49fc-9170-f249988d67b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","pd.set_option('display.max_columns', None)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":223,"status":"ok","timestamp":1729537375271,"user":{"displayName":"Hamid Usefi","userId":"10186078069234504560"},"user_tz":150},"id":"YY6cxwSILAWd","outputId":"41b0d62e-1178-4d65-fdad-8fce9f965026"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset Specifications:\n","Number of samples: 62\n","Number of columns: 2001\n","\n","Data Preview:\n","   0     1     2     3     4     5     6     7     8     9     ...  1991  \\\n","0   2.0   0.0   0.0   0.0  -2.0   0.0  -2.0   0.0   2.0   0.0  ...   0.0   \n","1   2.0   2.0   0.0   0.0  -2.0   0.0   0.0   0.0   2.0   0.0  ...   0.0   \n","2  -2.0   2.0   2.0   0.0  -2.0  -2.0  -2.0  -2.0  -2.0  -2.0  ...   0.0   \n","3   0.0   2.0   2.0   0.0  -2.0  -2.0  -2.0  -2.0   0.0   0.0  ...  -2.0   \n","4  -2.0  -2.0   0.0   0.0  -2.0  -2.0  -2.0   0.0  -2.0   0.0  ...   0.0   \n","\n","   1992  1993  1994  1995  1996  1997  1998  1999  2000  \n","0   0.0   0.0   0.0  -2.0   0.0   0.0   2.0  -2.0  -1.0  \n","1  -2.0   0.0  -2.0  -2.0   2.0   2.0   0.0  -2.0   1.0  \n","2  -2.0   2.0  -2.0   0.0  -2.0  -2.0  -2.0  -2.0  -1.0  \n","3  -2.0   2.0  -2.0  -2.0   0.0  -2.0   0.0  -2.0   1.0  \n","4  -2.0  -2.0  -2.0   0.0   0.0  -2.0  -2.0   0.0  -1.0  \n","\n","[5 rows x 2001 columns]\n","\n","Number of samples per label type:\n","2000\n","-1.0    40\n"," 1.0    22\n","Name: count, dtype: int64\n","\n","Checking if normalization is required...\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","\n","\n","dataset_path = '..\\data\\Datasets_SVFS\\colon.csv'\n","# dataset_path = '/content/drive/My Drive/data_code_DSCI6601/data/Datasets_SVFS/SMK_CAN_187.csv'\n","\n","if not os.path.exists(dataset_path):\n","    print(f\"File not found: {dataset_path}\")\n","else:\n","    # Read the dataset from a CSV file\n","    data = pd.read_csv(dataset_path, header=None)\n","\n","    # Print dataset specifications\n","    print(\"Dataset Specifications:\")\n","    print(f\"Number of samples: {data.shape[0]}\")\n","    print(f\"Number of columns: {data.shape[1]}\")\n","    # print(f\"Number of labels: {len(data.select_dtypes(include=['object', 'category']).columns)}\")\n","    # print(f\"Column names: {data.columns.tolist()}\")\n","    print(\"\\nData Preview:\")\n","    print(data.head())\n","\n","    # Check for normalization requirement (if any feature has variance > 1, normalization is needed)\n","    # feature_columns = data.select_dtypes(include=[np.number]).columns.tolist()\n","    # features = data[feature_columns]\n","    X = data.iloc[:, :-1]  # All columns except the last column\n","    y = data.iloc[:, -1]  # The last column is assumed to be the target label\n","\n","    # Print the number of samples per label type\n","    label_counts = y.value_counts()\n","    print(\"\\nNumber of samples per label type:\")\n","    print(label_counts)\n","    print(\"\\nChecking if normalization is required...\")"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":440,"status":"ok","timestamp":1729537384588,"user":{"displayName":"Hamid Usefi","userId":"10186078069234504560"},"user_tz":150},"id":"rXX_OEIgLJWv","outputId":"9fb6e18e-d532-46c0-a66b-3c0bf3c65194"},"outputs":[{"name":"stdout","output_type":"stream","text":["Selected 67 features out of 2000\n","Decision Tree Evaluation:\n","Confusion Matrix:\n","[[9 1]\n"," [5 4]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","     Class 0       0.64      0.90      0.75        10\n","     Class 1       0.80      0.44      0.57         9\n","\n","    accuracy                           0.68        19\n","   macro avg       0.72      0.67      0.66        19\n","weighted avg       0.72      0.68      0.67        19\n","\n","\n","Logistic Regression Evaluation:\n","Confusion Matrix:\n","[[9 1]\n"," [2 7]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","     Class 0       0.82      0.90      0.86        10\n","     Class 1       0.88      0.78      0.82         9\n","\n","    accuracy                           0.84        19\n","   macro avg       0.85      0.84      0.84        19\n","weighted avg       0.85      0.84      0.84        19\n","\n"]}],"source":["# Required imports\n","from sklearn.linear_model import Lasso\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression\n","\n","# Split data into features (X) and labels (y)\n","X = data.iloc[:, :-1]  # All columns except the last column\n","y = data.iloc[:, -1]   # The last column is assumed to be the target label\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Standardize the features (Lasso benefits from feature scaling)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Apply Lasso for feature selection (using alpha as regularization strength)\n","lasso2 = Lasso(alpha=0.01)  # You can adjust alpha for stronger/less regularization\n","lasso1 = Lasso(alpha = float(input('Enter ALPHA value:')))\n","# len(selected_features)\n","\n","lasso1.fit(X_train_scaled, y_train)\n","lasso2.fit(X_train_scaled, y_train)\n","# Get the selected features (non-zero coefficients)\n","selected_features = np.where(lasso1.coef_ != 0)[0]\n","selected_features2 = np.where(lasso2.coef_ != 0)[0]\n","print(f\"Selected {len(selected_features)} features out of {X_train.shape[1]}\")\n","\n","# Reduce the dataset to selected features\n","X_train_reduced = X_train_scaled[:, selected_features]\n","X_test_reduced = X_test_scaled[:, selected_features]\n","\n","# Apply Decision Tree Classifier on reduced data\n","dt_classifier = DecisionTreeClassifier(random_state=42)\n","dt_classifier.fit(X_train_reduced, y_train)\n","y_pred_dt = dt_classifier.predict(X_test_reduced)\n","\n","# Apply Logistic Regression on reduced data\n","logreg_classifier = LogisticRegression(random_state=42)\n","logreg_classifier.fit(X_train_reduced, y_train)\n","y_pred_logreg = logreg_classifier.predict(X_test_reduced)\n","\n","# Evaluation for Decision Tree\n","print(\"Decision Tree Evaluation:\")\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred_dt))\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred_dt, target_names=[\"Class 0\", \"Class 1\"]))\n","\n","# Evaluation for Logistic Regression\n","print(\"\\nLogistic Regression Evaluation:\")\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred_logreg))\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred_logreg, target_names=[\"Class 0\", \"Class 1\"]))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"q-45OMsyYYoU"},"source":[]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Length of Features with ALPHA Value 0.001: 67\n","Length of Features with ALPHA Value 0.01: 40\n","37 :  [2, 515, 779, 140, 1548, 142, 1039, 1676, 159, 160, 1057, 1569, 1059, 1317, 550, 553, 809, 179, 1717, 1334, 56, 697, 187, 1596, 1220, 1608, 723, 342, 599, 988, 869, 1255, 1643, 1901, 110, 123, 764]\n"]}],"source":["overlap_features = list(set(selected_features) & set(selected_features2))\n","# print(f\"Alpha value of lasso2: {lasso2.alpha}\")\n","# print(f\"Alpha value of lasso1: {lasso1.alpha}\")\n","print(f\"Length of Features with ALPHA Value {lasso1.alpha}:\",len(selected_features))\n","print(f\"Length of Features with ALPHA Value {lasso2.alpha}:\",len(selected_features2))\n","print(len(overlap_features),': ',overlap_features)"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33568,"status":"ok","timestamp":1729519496343,"user":{"displayName":"Hamid Usefi","userId":"10186078069234504560"},"user_tz":150},"id":"QdRSH6ypYY5N","outputId":"db8721da-589a-4468-8442-1c45da7cee15"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of selected features: 40\n","Selected features indices: [   2   84  123  140  244  266  512  515  553  599  641  723  738  764\n","  779  896  988 1039 1059 1063 1220 1255 1311 1334 1345 1386 1422 1441\n"," 1472 1484 1513 1548 1569 1581 1596 1643 1670 1770 1771 1899]\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.feature_selection import RFE\n","from sklearn.linear_model import LogisticRegression\n","\n","\n","\n","# Split data into features (X) and labels (y)\n","X = data.iloc[:, :-1]  # All columns except the last column\n","y = data.iloc[:, -1]   # The last column is assumed to be the target label\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Use Logistic Regression as the estimator for RFE\n","logreg = LogisticRegression(random_state=42)\n","\n","# Apply Recursive Feature Elimination (RFE)\n","rfe = RFE(estimator=logreg, n_features_to_select=40)  # Select top 10 features\n","rfe.fit(X_train_scaled, y_train)\n","\n","# Get the selected features\n","selected_features_rfe = np.where(rfe.support_ == True)[0]\n","\n","# Output the number of selected features\n","print(f\"Number of selected features: {len(selected_features_rfe)}\")\n","print(f\"Selected features indices: {selected_features_rfe}\")\n"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":219,"status":"ok","timestamp":1729519503571,"user":{"displayName":"Hamid Usefi","userId":"10186078069234504560"},"user_tz":150},"id":"EHSLnGtZZAu9","outputId":"e1377c35-f3d0-4c1b-a44d-1949f7cbd518"},"outputs":[{"name":"stdout","output_type":"stream","text":["Decision Tree Evaluation:\n","Confusion Matrix:\n","[[8 2]\n"," [3 6]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","     Class 0       0.73      0.80      0.76        10\n","     Class 1       0.75      0.67      0.71         9\n","\n","    accuracy                           0.74        19\n","   macro avg       0.74      0.73      0.73        19\n","weighted avg       0.74      0.74      0.74        19\n","\n","\n","Logistic Regression Evaluation:\n","Confusion Matrix:\n","[[8 2]\n"," [3 6]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","     Class 0       0.73      0.80      0.76        10\n","     Class 1       0.75      0.67      0.71         9\n","\n","    accuracy                           0.74        19\n","   macro avg       0.74      0.73      0.73        19\n","weighted avg       0.74      0.74      0.74        19\n","\n"]}],"source":["# Reduce the dataset to selected features\n","X_train_reduced = X_train_scaled[:, selected_features_rfe]\n","X_test_reduced = X_test_scaled[:, selected_features_rfe]\n","\n","# Apply Decision Tree Classifier on reduced data\n","dt_classifier = DecisionTreeClassifier(random_state=42)\n","dt_classifier.fit(X_train_reduced, y_train)\n","y_pred_dt = dt_classifier.predict(X_test_reduced)\n","\n","# Apply Logistic Regression on reduced data\n","logreg_classifier = LogisticRegression(random_state=42)\n","logreg_classifier.fit(X_train_reduced, y_train)\n","y_pred_logreg = logreg_classifier.predict(X_test_reduced)\n","\n","# Evaluation for Decision Tree\n","print(\"Decision Tree Evaluation:\")\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred_dt))\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred_dt, target_names=[\"Class 0\", \"Class 1\"]))\n","\n","# Evaluation for Logistic Regression\n","print(\"\\nLogistic Regression Evaluation:\")\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred_logreg))\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred_logreg, target_names=[\"Class 0\", \"Class 1\"]))"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1435,"status":"ok","timestamp":1729534493960,"user":{"displayName":"Hamid Usefi","userId":"10186078069234504560"},"user_tz":150},"id":"dtnTIqm0SQQE","outputId":"dd034fe6-6b77-4a5a-aee1-07e04786ea08"},"outputs":[{"name":"stdout","output_type":"stream","text":["Selected 47 features out of 2000\n","Decision Tree Evaluation:\n","Confusion Matrix:\n","[[9 1]\n"," [4 5]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","     Class 0       0.69      0.90      0.78        10\n","     Class 1       0.83      0.56      0.67         9\n","\n","    accuracy                           0.74        19\n","   macro avg       0.76      0.73      0.72        19\n","weighted avg       0.76      0.74      0.73        19\n","\n","\n","Logistic Regression Evaluation:\n","Confusion Matrix:\n","[[9 1]\n"," [2 7]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","     Class 0       0.82      0.90      0.86        10\n","     Class 1       0.88      0.78      0.82         9\n","\n","    accuracy                           0.84        19\n","   macro avg       0.85      0.84      0.84        19\n","weighted avg       0.85      0.84      0.84        19\n","\n","\n","Random Forest Evaluation:\n","Confusion Matrix:\n","[[8 2]\n"," [2 7]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","     Class 0       0.80      0.80      0.80        10\n","     Class 1       0.78      0.78      0.78         9\n","\n","    accuracy                           0.79        19\n","   macro avg       0.79      0.79      0.79        19\n","weighted avg       0.79      0.79      0.79        19\n","\n","\n","Support Vector Machine (SVM) Evaluation:\n","Confusion Matrix:\n","[[9 1]\n"," [2 7]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","     Class 0       0.82      0.90      0.86        10\n","     Class 1       0.88      0.78      0.82         9\n","\n","    accuracy                           0.84        19\n","   macro avg       0.85      0.84      0.84        19\n","weighted avg       0.85      0.84      0.84        19\n","\n"]}],"source":["# Required imports\n","from sklearn.linear_model import Lasso\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","\n","# Assuming data is loaded as pandas DataFrame\n","# Split data into features (X) and labels (y)\n","X = data.iloc[:, :-1]  # All columns except the last column\n","y = data.iloc[:, -1]   # The last column is assumed to be the target label\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Standardize the features (Lasso benefits from feature scaling)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Apply Lasso for feature selection (using alpha as regularization strength)\n","lasso = Lasso(alpha=0.005)  # You can adjust alpha for stronger/less regularization\n","lasso.fit(X_train_scaled, y_train)\n","\n","# Get the selected features (non-zero coefficients)\n","selected_features = np.where(lasso.coef_ != 0)[0]\n","print(f\"Selected {len(selected_features)} features out of {X_train.shape[1]}\")\n","\n","# Reduce the dataset to selected features\n","X_train_reduced = X_train_scaled[:, selected_features]\n","X_test_reduced = X_test_scaled[:, selected_features]\n","\n","### Apply Decision Tree Classifier on reduced data\n","dt_classifier = DecisionTreeClassifier(random_state=42)\n","dt_classifier.fit(X_train_reduced, y_train)\n","y_pred_dt = dt_classifier.predict(X_test_reduced)\n","\n","### Apply Logistic Regression on reduced data\n","logreg_classifier = LogisticRegression(random_state=42)\n","logreg_classifier.fit(X_train_reduced, y_train)\n","y_pred_logreg = logreg_classifier.predict(X_test_reduced)\n","\n","### Apply Random Forest Classifier on reduced data\n","rf_classifier = RandomForestClassifier(random_state=42)\n","rf_classifier.fit(X_train_reduced, y_train)\n","y_pred_rf = rf_classifier.predict(X_test_reduced)\n","\n","### Apply Support Vector Machine (SVM) Classifier on reduced data\n","svm_classifier = SVC(random_state=42)\n","svm_classifier.fit(X_train_reduced, y_train)\n","y_pred_svm = svm_classifier.predict(X_test_reduced)\n","\n","# Evaluation for Decision Tree\n","print(\"Decision Tree Evaluation:\")\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred_dt))\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred_dt, target_names=[\"Class 0\", \"Class 1\"]))\n","\n","# Evaluation for Logistic Regression\n","print(\"\\nLogistic Regression Evaluation:\")\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred_logreg))\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred_logreg, target_names=[\"Class 0\", \"Class 1\"]))\n","\n","# Evaluation for Random Forest\n","print(\"\\nRandom Forest Evaluation:\")\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred_rf))\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred_rf, target_names=[\"Class 0\", \"Class 1\"]))\n","\n","# Evaluation for SVM\n","print(\"\\nSupport Vector Machine (SVM) Evaluation:\")\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred_svm))\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred_svm, target_names=[\"Class 0\", \"Class 1\"]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNXNwYmqFXRfCxxYKugrWrJ","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":0}
